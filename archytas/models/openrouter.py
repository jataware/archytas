import pdb
import requests
import json
from typing import Generator, Literal, overload, TypedDict
from pathlib import Path

here = Path(__file__).parent


from .openrouter_model_names import ModelName
Role = Literal["user", "assistant", 'system']

class Message(TypedDict):
    role: Role
    content: str


class Model:
    """A simple class for talking to OpenRouter models directly via requests to the API"""
    def __init__(self, model:ModelName, openrouter_api_key:str):
        self.model = model
        self.openrouter_api_key = openrouter_api_key
    
    @overload
    def complete(self, messages: list[Message], stream:Literal[False]=False) -> str: ...
    @overload
    def complete(self, messages: list[Message], stream:Literal[True]) -> Generator[str, None, None]: ...
    def complete(self, messages: list[Message], stream:bool=False) -> str | Generator[str, None, None]:
        if stream:
            return self._streaming_complete(messages)
        else:
            return self._blocking_complete(messages)


    def _blocking_complete(self, messages: list[Message]) -> str:
        response = requests.post(
            url="https://openrouter.ai/api/v1/chat/completions",
            headers={"Authorization": f"Bearer {self.openrouter_api_key}"},
            data=json.dumps({"model": self.model, "messages": messages})
        )
        data = response.json()
        try:
            return data['choices'][0]['message']['content']
        except KeyError as e:
            raise ValueError(f"Unexpected response format: '{data}'. Please check the API response. {e}") from e
        except Exception as e:
            raise ValueError(f"An error occurred while processing the response: '{data}'. {e}") from e

    # TODO: should request timeout be a setting rather than hardcoded?
    def _streaming_complete(self, messages: list[Message]) -> Generator[str, None, None]:
        url = "https://openrouter.ai/api/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.openrouter_api_key}",
            "Content-Type": "application/json",
            "Accept": "text/event-stream",
        }
        payload = {"model": self.model, "messages": messages, "stream": True}

        with requests.post(url, headers=headers, json=payload, stream=True, timeout=(10, 60)) as r:
            r.raise_for_status()
            r.encoding = "utf-8"

            buf = []
            for line in r.iter_lines(decode_unicode=True, chunk_size=1024):
                if line is None:
                    continue
                
                if line.startswith("data: "):
                    buf.append(line[6:])
                    continue

                if line == "":  # end of one SSE event
                    if not buf:
                        continue
                    data = "\n".join(buf)
                    buf.clear()

                    if data == "[DONE]":
                        return

                    # parse the chunk and yield any content
                    try:
                        obj = json.loads(data)
                    except json.JSONDecodeError:
                        continue # wait for the next complete event
                    try:
                        content = obj["choices"][0]["delta"].get("content")
                        if content:
                            yield content
                    except KeyError as e:
                        raise ValueError(f"Unexpected response format: '{data}'. Please check the API response. {e}") from e
                    
                    continue

                
                # ignore other fields like "event:" / "id:" / comments




def list_openrouter_models(api_key:str) -> list[str]:
    """get the list of model names from OpenRouter API"""
    models = _get_openrouter_models(api_key)
    return [model['id'] for model in models]

def _get_openrouter_models(api_key: str) -> list[dict]:
    """Get the list of models (including all metadata) from OpenRouter API"""
    url = "https://openrouter.ai/api/v1/models"
    headers = {"Authorization": f"Bearer {api_key}"}
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        raise Exception(f"Error: {response.status_code}")

    data = response.json()['data']
    return data


# TODO: make a convenient project-level command for calling this function
def create_models_types_file(api_key: str, file:Path=here/'openrouter_model_names.py'):
    """
    Create a file with a type declaration `ModelName` containing all OpenRouter models.
    """
    models = list_openrouter_models(api_key)
    models.sort()
    print(f"Creating {file.relative_to(here.parent)} with {len(models)} models")
    lines = ',\n    '.join(f'"{model}"' for model in models)
    file.write_text(f'''\
# DO NOT EDIT THIS FILE MANUALLY
# This file is generated by calling `create_models_types_file()` in src/models.py
from typing import Literal
type ModelName = Literal[
    {lines}
]
''')



# if __name__ == "__main__":
#     import os
#     create_models_types_file(api_key=os.getenv('OPENROUTER_API_KEY'))




# from .model import Model, Message, Role
# from typing import Literal, Generator, overload

class Agent:
    """Basically just a model paired with message history tracking"""
    def __init__(self, model: Model):
        self.model = model
        self.messages: list[Message] = []

    def add_message(self, role: Role, content: str):
        message = Message(role=role, content=content)
        self.messages.append(message)
    
    def add_user_message(self, content: str):
        self.add_message(role='user', content=content)

    def add_assistant_message(self, content: str):
        self.add_message(role='assistant', content=content)

    def add_system_message(self, content: str):
        self.add_message(role='system', content=content)
    
    @overload
    def execute(self, stream:Literal[False]=False) -> str: ...
    @overload
    def execute(self, stream:Literal[True]) -> Generator[str, None, None]: ...
    def execute(self, stream:bool=False) -> str | Generator[str, None, None]:
        if stream:
            return self._streaming_execute()
        else:
            return self._blocking_execute()
    
    def _blocking_execute(self) -> str:
        result = self.model.complete(self.messages, stream=False)
        self.add_assistant_message(result)
        return result

    def _streaming_execute(self) -> Generator[str, None, None]:
        # stream the chunks while also capturing them
        result_chunks = []
        for chunk in self.model.complete(self.messages, stream=True):
            result_chunks.append(chunk)
            yield chunk
        
        # add the message to the history after streaming is done
        self.add_assistant_message(''.join(result_chunks))